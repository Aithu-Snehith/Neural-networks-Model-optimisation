{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V1_mobilenet_pruning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aithu-Snehith/Neural-networks-Model-optimisation/blob/master/V1_mobilenet_pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzIZswaDYHlu",
        "colab_type": "code",
        "outputId": "8f9c72ed-03e9-4c84-cbae-86c2ab1d6d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "! pip uninstall -y tensorflow\n",
        "! pip uninstall -y tf-nightly\n",
        "! pip install -U tf-nightly-gpu\n",
        "\n",
        "! pip install tensorflow-model-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "\u001b[33mWARNING: Skipping tf-nightly as it is not installed.\u001b[0m\n",
            "Requirement already up-to-date: tf-nightly-gpu in /usr/local/lib/python3.6/dist-packages (1.15.0.dev20190710)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (0.33.4)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (2.3.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: tf-estimator-nightly in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.14.0.dev2019071001)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: tb-nightly<1.15.0a0,>=1.14.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.14.0a20190614)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-gpu) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu) (41.0.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu) (0.15.4)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu) (3.1.1)\n",
            "Requirement already satisfied: tensorflow-model-optimization in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.16.4)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ-fTNLoYT70",
        "colab_type": "code",
        "outputId": "f354d698-7031-44e5-ed1e-155cb48bcbfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "%load_ext tensorboard\n",
        "import tensorboard\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import tempfile\n",
        "import zipfile\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0710 19:01:28.964322 140356938360704 module_wrapper.py:126] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/module_wrapper.py:153: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGDl25TCbVeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, GaussianDropout, GlobalAveragePooling2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.layers import *\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K201crCeK62A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'drive/path_to_data'\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 7\n",
        "epochs = 100\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 48, 48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZsazJCaOG67",
        "colab_type": "code",
        "outputId": "183c8501-9798-46f7-fd38-22cfbe3f1241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# x = np.load(path + 'snehith_fer2013_fdataX.npy')\n",
        "# y = np.load(path + 'snehith_fer2013_flabels.npy')\n",
        "\n",
        "# print(x.shape, y.shape)\n",
        "\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)\n",
        "\n",
        "# np.save(path + 'x_train.npy', x_train)\n",
        "# np.save(path + 'x_test.npy', x_test)\n",
        "# np.save(path + 'y_train.npy', y_train)\n",
        "# np.save(path + 'y_test.npy', y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35887, 48, 48, 1) (35887, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9B7L7OoRRO1",
        "colab_type": "code",
        "outputId": "04ac4fe4-b68a-4db5-ea4e-eb79b81ff5c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train = np.load(path + 'x_train.npy')\n",
        "x_test = np.load(path + 'x_test.npy')\n",
        "y_train = np.load(path + 'y_train.npy')\n",
        "y_test = np.load(path + 'y_test.npy')\n",
        "\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32298, 48, 48, 1) (3589, 48, 48, 1) (32298, 7) (3589, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw6h4_EmbQmn",
        "colab_type": "code",
        "outputId": "b8cc8def-859e-4940-9e5f-2a4395248f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def depth_conv_block(model, d, k, s):\n",
        "\tmodel.add(DepthwiseConv2D((k, k), strides=(s, s), padding='same', use_bias=False))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Activation('relu'))\n",
        "\tmodel.add(Conv2D(d, (1, 1), padding='same', use_bias=False))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Activation('relu'))\n",
        "\treturn model\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "# CONV\n",
        "model.add(Conv2D(int(32), (3, 3), padding='same', strides=(2, 2), input_shape=(img_rows,img_cols, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "# DEPTHWISE CONVS\n",
        "model = depth_conv_block(model, 64, 3, 1)\n",
        "\n",
        "model = depth_conv_block(model, 128, 3, 2)\n",
        "model = depth_conv_block(model, 128, 3, 1)\n",
        "\n",
        "model = depth_conv_block(model, 256, 3, 2)\n",
        "model = depth_conv_block(model, 256, 3, 1)\n",
        "\n",
        "model = depth_conv_block(model, 512, 3, 2)\n",
        "model = depth_conv_block(model, 512, 3, 1)\n",
        "model = depth_conv_block(model, 512, 3, 1)\n",
        "model = depth_conv_block(model, 512, 3, 1)\n",
        "model = depth_conv_block(model, 512, 3, 1)\n",
        "model = depth_conv_block(model, 512, 3, 1)\n",
        "\n",
        "model = depth_conv_block(model, 1024, 3, 2)\n",
        "model = depth_conv_block(model, 1024, 3, 1)\n",
        "# FLATTEN\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d (DepthwiseC (None, 24, 24, 32)        288       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 64)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_1 (Depthwis (None, 12, 12, 64)        576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 128)       8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_2 (Depthwis (None, 12, 12, 128)       1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 128)       16384     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_3 (Depthwis (None, 6, 6, 128)         1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 6, 6, 256)         32768     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_4 (Depthwis (None, 6, 6, 256)         2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 6, 6, 256)         65536     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_5 (Depthwis (None, 3, 3, 256)         2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 3, 3, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 3, 3, 512)         131072    \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_6 (Depthwis (None, 3, 3, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 3, 3, 512)         262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_7 (Depthwis (None, 3, 3, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 3, 3, 512)         262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_8 (Depthwis (None, 3, 3, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_9 (Depthwis (None, 3, 3, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 3, 3, 512)         262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_10 (Depthwi (None, 3, 3, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 3, 3, 512)         262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_11 (Depthwi (None, 2, 2, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 2, 2, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 2, 2, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_12 (Depthwi (None, 2, 2, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 2, 2, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 2, 2, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 2, 2, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 7)                 7175      \n",
            "=================================================================\n",
            "Total params: 3,235,495\n",
            "Trainable params: 3,213,607\n",
            "Non-trainable params: 21,888\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUqB8amcd8ot",
        "colab_type": "code",
        "outputId": "0948aa72-6384-428b-f9b0-10e4872c6983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir, profile_batch=0)]\n",
        "\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0710 19:09:28.942561 140356938360704 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1251: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0710 19:09:34.727003 140356938360704 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:460: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Apply a constraint manually following the optimizer update step.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 32298 samples, validate on 3589 samples\n",
            "Epoch 1/100\n",
            "32298/32298 [==============================] - 20s 621us/sample - loss: 1.8026 - acc: 0.2647 - val_loss: 1.8512 - val_acc: 0.2455\n",
            "Epoch 2/100\n",
            "32298/32298 [==============================] - 8s 245us/sample - loss: 1.6288 - acc: 0.3560 - val_loss: 1.9116 - val_acc: 0.1638\n",
            "Epoch 3/100\n",
            "32298/32298 [==============================] - 8s 244us/sample - loss: 1.5049 - acc: 0.4195 - val_loss: 1.5806 - val_acc: 0.3831\n",
            "Epoch 4/100\n",
            "32298/32298 [==============================] - 8s 245us/sample - loss: 1.4185 - acc: 0.4553 - val_loss: 1.9088 - val_acc: 0.3795\n",
            "Epoch 5/100\n",
            "32298/32298 [==============================] - 8s 245us/sample - loss: 1.3302 - acc: 0.4904 - val_loss: 1.7802 - val_acc: 0.4113\n",
            "Epoch 6/100\n",
            "32298/32298 [==============================] - 8s 245us/sample - loss: 1.2568 - acc: 0.5271 - val_loss: 1.7393 - val_acc: 0.3959\n",
            "Epoch 7/100\n",
            "32298/32298 [==============================] - 8s 245us/sample - loss: 1.1885 - acc: 0.5550 - val_loss: 1.7724 - val_acc: 0.3979\n",
            "Epoch 8/100\n",
            "32298/32298 [==============================] - 8s 246us/sample - loss: 1.1177 - acc: 0.5829 - val_loss: 1.5940 - val_acc: 0.4411\n",
            "Epoch 9/100\n",
            "32298/32298 [==============================] - 8s 247us/sample - loss: 1.0523 - acc: 0.6092 - val_loss: 1.7535 - val_acc: 0.4595\n",
            "Epoch 10/100\n",
            "32298/32298 [==============================] - 8s 249us/sample - loss: 0.9895 - acc: 0.6343 - val_loss: 2.0012 - val_acc: 0.4296\n",
            "Epoch 11/100\n",
            "32298/32298 [==============================] - 8s 248us/sample - loss: 0.9074 - acc: 0.6675 - val_loss: 1.8684 - val_acc: 0.4603\n",
            "Epoch 12/100\n",
            "32298/32298 [==============================] - 8s 249us/sample - loss: 0.8471 - acc: 0.6893 - val_loss: 1.7774 - val_acc: 0.4531\n",
            "Epoch 13/100\n",
            "32298/32298 [==============================] - 8s 250us/sample - loss: 0.7798 - acc: 0.7187 - val_loss: 1.8006 - val_acc: 0.4547\n",
            "Epoch 14/100\n",
            "32298/32298 [==============================] - 8s 249us/sample - loss: 0.7071 - acc: 0.7470 - val_loss: 1.8782 - val_acc: 0.4466\n",
            "Epoch 15/100\n",
            "32298/32298 [==============================] - 8s 249us/sample - loss: 0.6431 - acc: 0.7695 - val_loss: 2.2978 - val_acc: 0.4650\n",
            "Epoch 16/100\n",
            "32298/32298 [==============================] - 8s 251us/sample - loss: 0.5818 - acc: 0.7912 - val_loss: 2.0233 - val_acc: 0.4820\n",
            "Epoch 17/100\n",
            "32298/32298 [==============================] - 8s 251us/sample - loss: 0.5188 - acc: 0.8141 - val_loss: 2.2544 - val_acc: 0.4500\n",
            "Epoch 18/100\n",
            "32298/32298 [==============================] - 8s 251us/sample - loss: 0.4757 - acc: 0.8304 - val_loss: 2.1643 - val_acc: 0.4687\n",
            "Epoch 19/100\n",
            "32298/32298 [==============================] - 8s 251us/sample - loss: 0.4156 - acc: 0.8544 - val_loss: 2.5700 - val_acc: 0.4374\n",
            "Epoch 20/100\n",
            "32298/32298 [==============================] - 8s 251us/sample - loss: 0.3785 - acc: 0.8657 - val_loss: 2.3081 - val_acc: 0.4634\n",
            "Epoch 21/100\n",
            "32298/32298 [==============================] - 8s 251us/sample - loss: 0.3557 - acc: 0.8746 - val_loss: 2.2851 - val_acc: 0.4703\n",
            "Epoch 22/100\n",
            "32298/32298 [==============================] - 8s 252us/sample - loss: 0.3078 - acc: 0.8925 - val_loss: 2.4103 - val_acc: 0.4742\n",
            "Epoch 23/100\n",
            "32298/32298 [==============================] - 8s 251us/sample - loss: 0.2991 - acc: 0.8947 - val_loss: 2.4812 - val_acc: 0.4893\n",
            "Epoch 24/100\n",
            "32298/32298 [==============================] - 8s 253us/sample - loss: 0.2709 - acc: 0.9030 - val_loss: 2.5234 - val_acc: 0.4812\n",
            "Epoch 25/100\n",
            "32298/32298 [==============================] - 8s 252us/sample - loss: 0.2525 - acc: 0.9120 - val_loss: 2.4419 - val_acc: 0.4817\n",
            "Epoch 26/100\n",
            "32298/32298 [==============================] - 8s 252us/sample - loss: 0.2428 - acc: 0.9169 - val_loss: 2.6868 - val_acc: 0.4784\n",
            "Epoch 27/100\n",
            "32298/32298 [==============================] - 8s 254us/sample - loss: 0.2282 - acc: 0.9211 - val_loss: 2.4787 - val_acc: 0.4848\n",
            "Epoch 28/100\n",
            "32298/32298 [==============================] - 8s 253us/sample - loss: 0.2180 - acc: 0.9252 - val_loss: 2.4558 - val_acc: 0.4823\n",
            "Epoch 29/100\n",
            "32298/32298 [==============================] - 8s 253us/sample - loss: 0.2014 - acc: 0.9317 - val_loss: 2.5639 - val_acc: 0.4865\n",
            "Epoch 30/100\n",
            "32298/32298 [==============================] - 8s 252us/sample - loss: 0.2036 - acc: 0.9296 - val_loss: 2.4064 - val_acc: 0.4868\n",
            "Epoch 31/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.1981 - acc: 0.9317 - val_loss: 2.6467 - val_acc: 0.4948\n",
            "Epoch 32/100\n",
            "32298/32298 [==============================] - 8s 254us/sample - loss: 0.1920 - acc: 0.9334 - val_loss: 2.6714 - val_acc: 0.4926\n",
            "Epoch 33/100\n",
            "32298/32298 [==============================] - 8s 252us/sample - loss: 0.1738 - acc: 0.9400 - val_loss: 2.6989 - val_acc: 0.4731\n",
            "Epoch 34/100\n",
            "32298/32298 [==============================] - 8s 253us/sample - loss: 0.1690 - acc: 0.9427 - val_loss: 2.8151 - val_acc: 0.4795\n",
            "Epoch 35/100\n",
            "32298/32298 [==============================] - 8s 253us/sample - loss: 0.1820 - acc: 0.9367 - val_loss: 2.6652 - val_acc: 0.5010\n",
            "Epoch 36/100\n",
            "32298/32298 [==============================] - 8s 254us/sample - loss: 0.1679 - acc: 0.9426 - val_loss: 2.6048 - val_acc: 0.4974\n",
            "Epoch 37/100\n",
            "32298/32298 [==============================] - 8s 254us/sample - loss: 0.1610 - acc: 0.9451 - val_loss: 2.5917 - val_acc: 0.4943\n",
            "Epoch 38/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.1589 - acc: 0.9446 - val_loss: 2.7009 - val_acc: 0.4865\n",
            "Epoch 39/100\n",
            "32298/32298 [==============================] - 8s 258us/sample - loss: 0.1464 - acc: 0.9499 - val_loss: 2.7283 - val_acc: 0.4862\n",
            "Epoch 40/100\n",
            "32298/32298 [==============================] - 8s 254us/sample - loss: 0.1427 - acc: 0.9515 - val_loss: 2.7858 - val_acc: 0.5024\n",
            "Epoch 41/100\n",
            "32298/32298 [==============================] - 8s 254us/sample - loss: 0.1429 - acc: 0.9504 - val_loss: 2.6457 - val_acc: 0.4971\n",
            "Epoch 42/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.1475 - acc: 0.9488 - val_loss: 2.5857 - val_acc: 0.5035\n",
            "Epoch 43/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.1450 - acc: 0.9488 - val_loss: 2.8528 - val_acc: 0.4673\n",
            "Epoch 44/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.1286 - acc: 0.9571 - val_loss: 2.9266 - val_acc: 0.4709\n",
            "Epoch 45/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.1339 - acc: 0.9526 - val_loss: 2.8594 - val_acc: 0.4932\n",
            "Epoch 46/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.1256 - acc: 0.9563 - val_loss: 2.7555 - val_acc: 0.5021\n",
            "Epoch 47/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.1278 - acc: 0.9558 - val_loss: 2.7109 - val_acc: 0.5015\n",
            "Epoch 48/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.1352 - acc: 0.9530 - val_loss: 2.7597 - val_acc: 0.5091\n",
            "Epoch 49/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.1172 - acc: 0.9594 - val_loss: 2.7397 - val_acc: 0.5040\n",
            "Epoch 50/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.1245 - acc: 0.9580 - val_loss: 2.6417 - val_acc: 0.4990\n",
            "Epoch 51/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.1147 - acc: 0.9593 - val_loss: 2.8452 - val_acc: 0.4968\n",
            "Epoch 52/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.1186 - acc: 0.9600 - val_loss: 2.7905 - val_acc: 0.4932\n",
            "Epoch 53/100\n",
            "32298/32298 [==============================] - 8s 254us/sample - loss: 0.1157 - acc: 0.9595 - val_loss: 2.8201 - val_acc: 0.4901\n",
            "Epoch 54/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.1151 - acc: 0.9600 - val_loss: 2.8499 - val_acc: 0.5026\n",
            "Epoch 55/100\n",
            "32298/32298 [==============================] - 8s 254us/sample - loss: 0.1141 - acc: 0.9601 - val_loss: 2.8406 - val_acc: 0.4960\n",
            "Epoch 56/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.1030 - acc: 0.9644 - val_loss: 2.8695 - val_acc: 0.5035\n",
            "Epoch 57/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.1004 - acc: 0.9650 - val_loss: 2.7833 - val_acc: 0.4993\n",
            "Epoch 58/100\n",
            "32298/32298 [==============================] - 8s 258us/sample - loss: 0.1044 - acc: 0.9640 - val_loss: 3.1764 - val_acc: 0.4781\n",
            "Epoch 59/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0944 - acc: 0.9669 - val_loss: 2.8882 - val_acc: 0.4848\n",
            "Epoch 60/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.1021 - acc: 0.9641 - val_loss: 3.4503 - val_acc: 0.4581\n",
            "Epoch 61/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.1029 - acc: 0.9629 - val_loss: 2.9707 - val_acc: 0.4787\n",
            "Epoch 62/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.1015 - acc: 0.9632 - val_loss: 2.9693 - val_acc: 0.5001\n",
            "Epoch 63/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.0873 - acc: 0.9699 - val_loss: 2.8595 - val_acc: 0.5021\n",
            "Epoch 64/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0963 - acc: 0.9658 - val_loss: 2.8567 - val_acc: 0.5024\n",
            "Epoch 65/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0961 - acc: 0.9671 - val_loss: 2.7467 - val_acc: 0.5099\n",
            "Epoch 66/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.0908 - acc: 0.9678 - val_loss: 2.8907 - val_acc: 0.5060\n",
            "Epoch 67/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.0903 - acc: 0.9684 - val_loss: 2.9241 - val_acc: 0.5063\n",
            "Epoch 68/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0875 - acc: 0.9688 - val_loss: 2.9326 - val_acc: 0.5054\n",
            "Epoch 69/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0844 - acc: 0.9700 - val_loss: 3.0012 - val_acc: 0.5091\n",
            "Epoch 70/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0818 - acc: 0.9713 - val_loss: 2.9654 - val_acc: 0.4968\n",
            "Epoch 71/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0866 - acc: 0.9693 - val_loss: 3.0022 - val_acc: 0.5091\n",
            "Epoch 72/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0803 - acc: 0.9719 - val_loss: 2.9138 - val_acc: 0.5052\n",
            "Epoch 73/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0883 - acc: 0.9683 - val_loss: 3.0551 - val_acc: 0.5155\n",
            "Epoch 74/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0805 - acc: 0.9711 - val_loss: 2.8685 - val_acc: 0.5035\n",
            "Epoch 75/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0840 - acc: 0.9699 - val_loss: 2.9377 - val_acc: 0.5121\n",
            "Epoch 76/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0691 - acc: 0.9758 - val_loss: 3.0050 - val_acc: 0.5152\n",
            "Epoch 77/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0787 - acc: 0.9715 - val_loss: 3.0658 - val_acc: 0.5088\n",
            "Epoch 78/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0779 - acc: 0.9724 - val_loss: 2.9974 - val_acc: 0.5035\n",
            "Epoch 79/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0725 - acc: 0.9745 - val_loss: 3.0118 - val_acc: 0.5049\n",
            "Epoch 80/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0759 - acc: 0.9731 - val_loss: 3.0853 - val_acc: 0.5077\n",
            "Epoch 81/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.0740 - acc: 0.9737 - val_loss: 3.0016 - val_acc: 0.4926\n",
            "Epoch 82/100\n",
            "32298/32298 [==============================] - 8s 259us/sample - loss: 0.0698 - acc: 0.9753 - val_loss: 3.0331 - val_acc: 0.5077\n",
            "Epoch 83/100\n",
            "32298/32298 [==============================] - 8s 258us/sample - loss: 0.0764 - acc: 0.9741 - val_loss: 3.0939 - val_acc: 0.4985\n",
            "Epoch 84/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0691 - acc: 0.9754 - val_loss: 3.0731 - val_acc: 0.4918\n",
            "Epoch 85/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0672 - acc: 0.9764 - val_loss: 3.1030 - val_acc: 0.4921\n",
            "Epoch 86/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0726 - acc: 0.9740 - val_loss: 3.0811 - val_acc: 0.5113\n",
            "Epoch 87/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0636 - acc: 0.9780 - val_loss: 3.1749 - val_acc: 0.5052\n",
            "Epoch 88/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0793 - acc: 0.9728 - val_loss: 3.0516 - val_acc: 0.5038\n",
            "Epoch 89/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0624 - acc: 0.9779 - val_loss: 3.0570 - val_acc: 0.4887\n",
            "Epoch 90/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0669 - acc: 0.9762 - val_loss: 3.0421 - val_acc: 0.5269\n",
            "Epoch 91/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0642 - acc: 0.9776 - val_loss: 2.9941 - val_acc: 0.5096\n",
            "Epoch 92/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0641 - acc: 0.9769 - val_loss: 3.2129 - val_acc: 0.4932\n",
            "Epoch 93/100\n",
            "32298/32298 [==============================] - 8s 255us/sample - loss: 0.0779 - acc: 0.9728 - val_loss: 2.9768 - val_acc: 0.5074\n",
            "Epoch 94/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0647 - acc: 0.9760 - val_loss: 2.9914 - val_acc: 0.4929\n",
            "Epoch 95/100\n",
            "32298/32298 [==============================] - 8s 258us/sample - loss: 0.0626 - acc: 0.9778 - val_loss: 3.0707 - val_acc: 0.5157\n",
            "Epoch 96/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0649 - acc: 0.9774 - val_loss: 3.0934 - val_acc: 0.5074\n",
            "Epoch 97/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0590 - acc: 0.9787 - val_loss: 3.1292 - val_acc: 0.5026\n",
            "Epoch 98/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0588 - acc: 0.9791 - val_loss: 3.1080 - val_acc: 0.5065\n",
            "Epoch 99/100\n",
            "32298/32298 [==============================] - 8s 256us/sample - loss: 0.0591 - acc: 0.9791 - val_loss: 3.0438 - val_acc: 0.5049\n",
            "Epoch 100/100\n",
            "32298/32298 [==============================] - 8s 257us/sample - loss: 0.0637 - acc: 0.9773 - val_loss: 2.8871 - val_acc: 0.5194\n",
            "Test loss: 2.887106343344016\n",
            "Test accuracy: 0.5193647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOtjblmTeCAC",
        "colab_type": "code",
        "outputId": "ab9c53e0-dd66-4754-c52d-6437d6b8fc40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Backend agnostic way to save/restore models\n",
        "keras_file = path + 'raw_mobilenet_model.h5'\n",
        "print('Saving model to: ', keras_file)\n",
        "tf.keras.models.save_model(model, keras_file, include_optimizer=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to:  drive/My Drive/interns/Kpit/raw_mobilenet_model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQN0_c3WfFdM",
        "colab_type": "code",
        "outputId": "11dc3f12-8cf1-4ab3-e954-57a4c3822e79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
        "\n",
        "epochs = 100\n",
        "num_train_samples = x_train.shape[0]\n",
        "end_step = np.ceil(1.0 * num_train_samples / batch_size).astype(np.int32) * epochs\n",
        "print('End step: ' + str(end_step))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "End step: 25300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCH4kuy2fagr",
        "colab_type": "code",
        "outputId": "9306c474-bb79-4ced-9e95-9c9791ba810d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pruning_params = {\n",
        "      'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                   final_sparsity=0.90,\n",
        "                                                   begin_step=2000,\n",
        "                                                   end_step=end_step,\n",
        "                                                   frequency=100)\n",
        "}\n",
        "\n",
        "\n",
        "def prune_depth_conv_block(model, d, k, s):\n",
        "\tmodel.add(sparsity.prune_low_magnitude(DepthwiseConv2D((k, k), strides=(s, s), padding='same', use_bias=False), **pruning_params))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Activation('relu'))\n",
        "\tmodel.add(sparsity.prune_low_magnitude(Conv2D(d, (1, 1), padding='same', use_bias=False), **pruning_params))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Activation('relu'))\n",
        "\treturn model\n",
        "\n",
        "\n",
        "pr_model = Sequential()\n",
        "# CONV\n",
        "pr_model.add(sparsity.prune_low_magnitude(Conv2D(int(32), (3, 3), padding='same', strides=(2, 2), input_shape=(img_rows,img_cols, 1)), **pruning_params))\n",
        "pr_model.add(BatchNormalization())\n",
        "pr_model.add(Activation('relu'))\n",
        "# DEPTHWISE CONVS\n",
        "pr_model = prune_depth_conv_block(pr_model, 64, 3, 1)\n",
        "\n",
        "pr_model = prune_depth_conv_block(pr_model, 128, 3, 2)\n",
        "pr_model = prune_depth_conv_block(pr_model, 128, 3, 1)\n",
        "\n",
        "pr_model = prune_depth_conv_block(pr_model, 256, 3, 2)\n",
        "pr_model = prune_depth_conv_block(pr_model, 256, 3, 1)\n",
        "\n",
        "pr_model = prune_depth_conv_block(pr_model, 512, 3, 2)\n",
        "pr_model = prune_depth_conv_block(pr_model, 512, 3, 1)\n",
        "pr_model = prune_depth_conv_block(pr_model, 512, 3, 1)\n",
        "pr_model = prune_depth_conv_block(pr_model, 512, 3, 1)\n",
        "pr_model = prune_depth_conv_block(pr_model, 512, 3, 1)\n",
        "pr_model = prune_depth_conv_block(pr_model, 512, 3, 1)\n",
        "\n",
        "pr_model = prune_depth_conv_block(pr_model, 1024, 3, 2)\n",
        "pr_model = prune_depth_conv_block(pr_model, 1024, 3, 1)\n",
        "# FLATTEN\n",
        "pr_model.add(GlobalAveragePooling2D())\n",
        "pr_model.add(sparsity.prune_low_magnitude(Dense(num_classes, activation='softmax'), **pruning_params))\n",
        "pr_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0710 19:25:13.846943 140356938360704 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:185: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "W0710 19:25:14.600370 140356938360704 module_wrapper.py:126] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/module_wrapper.py:153: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
            "\n",
            "W0710 19:25:14.791079 140356938360704 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_schedule.py:240: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "prune_low_magnitude_conv2d_1 (None, 24, 24, 32)        610       \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 24, 24, 32)        289       \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 24, 24, 64)        4098      \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 12, 12, 64)        577       \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 12, 12, 128)       16386     \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 12, 12, 128)       1153      \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 12, 12, 128)       32770     \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 6, 6, 128)         1153      \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 6, 6, 256)         65538     \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 6, 6, 256)         2305      \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_1 (None, 6, 6, 256)         131074    \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 3, 3, 256)         2305      \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 3, 3, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 3, 3, 512)         262146    \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 3, 3, 512)         4609      \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 3, 3, 512)         524290    \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 3, 3, 512)         4609      \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 3, 3, 512)         524290    \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 3, 3, 512)         4609      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 3, 3, 512)         524290    \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 3, 3, 512)         4609      \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 3, 3, 512)         524290    \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 3, 3, 512)         4609      \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 3, 3, 512)         524290    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 2, 2, 512)         4609      \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 2, 2, 1024)        1048578   \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 2, 2, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_depthwis (None, 2, 2, 1024)        9217      \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 2, 2, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_conv2d_2 (None, 2, 2, 1024)        2097154   \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 2, 2, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "prune_low_magnitude_dense_1  (None, 7)                 14345     \n",
            "=================================================================\n",
            "Total params: 6,382,578\n",
            "Trainable params: 3,213,607\n",
            "Non-trainable params: 3,168,971\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4acdDlbiM9z",
        "colab_type": "code",
        "outputId": "5d205a0c-3e63-4d99-f67c-8c44bf38a88a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pr_model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Add a pruning step callback to peg the pruning step to the optimizer's\n",
        "# step. Also add a callback to add pruning summaries to tensorboard\n",
        "callbacks = [\n",
        "    sparsity.UpdatePruningStep(),\n",
        "    sparsity.PruningSummaries(log_dir=logdir, profile_batch=0)\n",
        "]\n",
        "\n",
        "pr_model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=100,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = pr_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32298 samples, validate on 3589 samples\n",
            "Epoch 1/100\n",
            "32298/32298 [==============================] - 22s 697us/sample - loss: 1.7932 - acc: 0.2644 - val_loss: 1.8703 - val_acc: 0.1702\n",
            "Epoch 2/100\n",
            "32298/32298 [==============================] - 12s 381us/sample - loss: 1.6248 - acc: 0.3606 - val_loss: 1.9531 - val_acc: 0.1638\n",
            "Epoch 3/100\n",
            "32298/32298 [==============================] - 12s 379us/sample - loss: 1.5112 - acc: 0.4161 - val_loss: 1.5685 - val_acc: 0.3831\n",
            "Epoch 4/100\n",
            "32298/32298 [==============================] - 12s 382us/sample - loss: 1.4238 - acc: 0.4551 - val_loss: 1.8210 - val_acc: 0.3667\n",
            "Epoch 5/100\n",
            "32298/32298 [==============================] - 12s 383us/sample - loss: 1.3390 - acc: 0.4902 - val_loss: 1.9022 - val_acc: 0.3937\n",
            "Epoch 6/100\n",
            "32298/32298 [==============================] - 12s 380us/sample - loss: 1.2524 - acc: 0.5302 - val_loss: 1.8510 - val_acc: 0.3784\n",
            "Epoch 7/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 1.1904 - acc: 0.5541 - val_loss: 1.9123 - val_acc: 0.3973\n",
            "Epoch 8/100\n",
            "32298/32298 [==============================] - 12s 386us/sample - loss: 1.1442 - acc: 0.5717 - val_loss: 1.6719 - val_acc: 0.3279\n",
            "Epoch 9/100\n",
            "32298/32298 [==============================] - 13s 389us/sample - loss: 1.2935 - acc: 0.5077 - val_loss: 1.4569 - val_acc: 0.4558\n",
            "Epoch 10/100\n",
            "32298/32298 [==============================] - 12s 382us/sample - loss: 1.1474 - acc: 0.5703 - val_loss: 1.4798 - val_acc: 0.4600\n",
            "Epoch 11/100\n",
            "32298/32298 [==============================] - 12s 383us/sample - loss: 1.0563 - acc: 0.6070 - val_loss: 1.5319 - val_acc: 0.4642\n",
            "Epoch 12/100\n",
            "32298/32298 [==============================] - 12s 377us/sample - loss: 0.9860 - acc: 0.6365 - val_loss: 1.7910 - val_acc: 0.4361\n",
            "Epoch 13/100\n",
            "32298/32298 [==============================] - 12s 377us/sample - loss: 0.9231 - acc: 0.6610 - val_loss: 1.6790 - val_acc: 0.4642\n",
            "Epoch 14/100\n",
            "32298/32298 [==============================] - 12s 382us/sample - loss: 0.8710 - acc: 0.6769 - val_loss: 1.6332 - val_acc: 0.4876\n",
            "Epoch 15/100\n",
            "32298/32298 [==============================] - 12s 380us/sample - loss: 0.8035 - acc: 0.7053 - val_loss: 1.7622 - val_acc: 0.4739\n",
            "Epoch 16/100\n",
            "32298/32298 [==============================] - 12s 382us/sample - loss: 0.7621 - acc: 0.7211 - val_loss: 1.7025 - val_acc: 0.4857\n",
            "Epoch 17/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.7068 - acc: 0.7401 - val_loss: 2.2257 - val_acc: 0.4032\n",
            "Epoch 18/100\n",
            "32298/32298 [==============================] - 13s 388us/sample - loss: 0.6604 - acc: 0.7608 - val_loss: 1.9031 - val_acc: 0.4678\n",
            "Epoch 19/100\n",
            "32298/32298 [==============================] - 12s 386us/sample - loss: 0.5979 - acc: 0.7834 - val_loss: 1.9437 - val_acc: 0.4759\n",
            "Epoch 20/100\n",
            "32298/32298 [==============================] - 12s 378us/sample - loss: 0.5549 - acc: 0.7996 - val_loss: 2.2188 - val_acc: 0.4536\n",
            "Epoch 21/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.5230 - acc: 0.8095 - val_loss: 2.1200 - val_acc: 0.4567\n",
            "Epoch 22/100\n",
            "32298/32298 [==============================] - 12s 381us/sample - loss: 0.5054 - acc: 0.8188 - val_loss: 2.0419 - val_acc: 0.4851\n",
            "Epoch 23/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.4813 - acc: 0.8249 - val_loss: 2.0903 - val_acc: 0.4848\n",
            "Epoch 24/100\n",
            "32298/32298 [==============================] - 12s 387us/sample - loss: 0.4325 - acc: 0.8431 - val_loss: 2.2209 - val_acc: 0.4726\n",
            "Epoch 25/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.4671 - acc: 0.8305 - val_loss: 2.2077 - val_acc: 0.4815\n",
            "Epoch 26/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.4340 - acc: 0.8434 - val_loss: 2.1232 - val_acc: 0.4976\n",
            "Epoch 27/100\n",
            "32298/32298 [==============================] - 13s 388us/sample - loss: 0.3911 - acc: 0.8593 - val_loss: 2.2438 - val_acc: 0.4962\n",
            "Epoch 28/100\n",
            "32298/32298 [==============================] - 12s 381us/sample - loss: 0.3783 - acc: 0.8630 - val_loss: 2.5741 - val_acc: 0.4639\n",
            "Epoch 29/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.3699 - acc: 0.8662 - val_loss: 2.5096 - val_acc: 0.4469\n",
            "Epoch 30/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.3683 - acc: 0.8682 - val_loss: 2.3073 - val_acc: 0.4843\n",
            "Epoch 31/100\n",
            "32298/32298 [==============================] - 12s 382us/sample - loss: 0.3548 - acc: 0.8732 - val_loss: 2.2786 - val_acc: 0.4968\n",
            "Epoch 32/100\n",
            "32298/32298 [==============================] - 12s 387us/sample - loss: 0.3275 - acc: 0.8827 - val_loss: 2.3795 - val_acc: 0.4957\n",
            "Epoch 33/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.3465 - acc: 0.8740 - val_loss: 2.2482 - val_acc: 0.4943\n",
            "Epoch 34/100\n",
            "32298/32298 [==============================] - 12s 387us/sample - loss: 0.3186 - acc: 0.8846 - val_loss: 2.4633 - val_acc: 0.4759\n",
            "Epoch 35/100\n",
            "32298/32298 [==============================] - 12s 382us/sample - loss: 0.3210 - acc: 0.8854 - val_loss: 2.5164 - val_acc: 0.4957\n",
            "Epoch 36/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.3161 - acc: 0.8856 - val_loss: 2.3646 - val_acc: 0.4742\n",
            "Epoch 37/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.3383 - acc: 0.8773 - val_loss: 2.3792 - val_acc: 0.4890\n",
            "Epoch 38/100\n",
            "32298/32298 [==============================] - 12s 387us/sample - loss: 0.3402 - acc: 0.8775 - val_loss: 2.2413 - val_acc: 0.4965\n",
            "Epoch 39/100\n",
            "32298/32298 [==============================] - 12s 382us/sample - loss: 0.3172 - acc: 0.8854 - val_loss: 2.3346 - val_acc: 0.4954\n",
            "Epoch 40/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.3001 - acc: 0.8921 - val_loss: 2.3479 - val_acc: 0.5013\n",
            "Epoch 41/100\n",
            "32298/32298 [==============================] - 12s 381us/sample - loss: 0.2908 - acc: 0.8942 - val_loss: 2.4987 - val_acc: 0.5004\n",
            "Epoch 42/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.2758 - acc: 0.8998 - val_loss: 2.6073 - val_acc: 0.4985\n",
            "Epoch 43/100\n",
            "32298/32298 [==============================] - 12s 386us/sample - loss: 0.2811 - acc: 0.8988 - val_loss: 2.5020 - val_acc: 0.4904\n",
            "Epoch 44/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.2881 - acc: 0.8958 - val_loss: 2.4530 - val_acc: 0.4996\n",
            "Epoch 45/100\n",
            "32298/32298 [==============================] - 12s 382us/sample - loss: 0.2953 - acc: 0.8932 - val_loss: 2.4992 - val_acc: 0.4985\n",
            "Epoch 46/100\n",
            "32298/32298 [==============================] - 12s 383us/sample - loss: 0.3092 - acc: 0.8860 - val_loss: 2.5185 - val_acc: 0.4748\n",
            "Epoch 47/100\n",
            "32298/32298 [==============================] - 12s 380us/sample - loss: 0.2868 - acc: 0.8966 - val_loss: 2.4854 - val_acc: 0.4876\n",
            "Epoch 48/100\n",
            "32298/32298 [==============================] - 13s 388us/sample - loss: 0.3135 - acc: 0.8879 - val_loss: 2.4359 - val_acc: 0.4776\n",
            "Epoch 49/100\n",
            "32298/32298 [==============================] - 12s 380us/sample - loss: 0.3013 - acc: 0.8911 - val_loss: 2.3949 - val_acc: 0.5013\n",
            "Epoch 50/100\n",
            "32298/32298 [==============================] - 13s 392us/sample - loss: 0.2948 - acc: 0.8939 - val_loss: 2.4258 - val_acc: 0.4862\n",
            "Epoch 51/100\n",
            "32298/32298 [==============================] - 12s 381us/sample - loss: 0.3039 - acc: 0.8912 - val_loss: 2.4393 - val_acc: 0.4820\n",
            "Epoch 52/100\n",
            "32298/32298 [==============================] - 12s 380us/sample - loss: 0.2853 - acc: 0.8964 - val_loss: 2.4308 - val_acc: 0.5085\n",
            "Epoch 53/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.2781 - acc: 0.9009 - val_loss: 2.4736 - val_acc: 0.4890\n",
            "Epoch 54/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.2905 - acc: 0.8958 - val_loss: 2.5139 - val_acc: 0.4887\n",
            "Epoch 55/100\n",
            "32298/32298 [==============================] - 12s 381us/sample - loss: 0.2928 - acc: 0.8928 - val_loss: 2.4546 - val_acc: 0.4879\n",
            "Epoch 56/100\n",
            "32298/32298 [==============================] - 12s 386us/sample - loss: 0.2965 - acc: 0.8914 - val_loss: 2.4471 - val_acc: 0.4915\n",
            "Epoch 57/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.2879 - acc: 0.8968 - val_loss: 2.4481 - val_acc: 0.4929\n",
            "Epoch 58/100\n",
            "32298/32298 [==============================] - 13s 390us/sample - loss: 0.2738 - acc: 0.9024 - val_loss: 2.4292 - val_acc: 0.5018\n",
            "Epoch 59/100\n",
            "32298/32298 [==============================] - 12s 383us/sample - loss: 0.2865 - acc: 0.8951 - val_loss: 2.3902 - val_acc: 0.5035\n",
            "Epoch 60/100\n",
            "32298/32298 [==============================] - 12s 378us/sample - loss: 0.2849 - acc: 0.8985 - val_loss: 2.4395 - val_acc: 0.4982\n",
            "Epoch 61/100\n",
            "32298/32298 [==============================] - 12s 379us/sample - loss: 0.2824 - acc: 0.8989 - val_loss: 2.4525 - val_acc: 0.5026\n",
            "Epoch 62/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.2703 - acc: 0.9030 - val_loss: 2.5314 - val_acc: 0.5013\n",
            "Epoch 63/100\n",
            "32298/32298 [==============================] - 13s 389us/sample - loss: 0.2708 - acc: 0.9044 - val_loss: 2.6183 - val_acc: 0.4990\n",
            "Epoch 64/100\n",
            "32298/32298 [==============================] - 12s 380us/sample - loss: 0.2518 - acc: 0.9095 - val_loss: 2.6193 - val_acc: 0.4999\n",
            "Epoch 65/100\n",
            "32298/32298 [==============================] - 12s 381us/sample - loss: 0.2679 - acc: 0.9041 - val_loss: 2.4992 - val_acc: 0.5118\n",
            "Epoch 66/100\n",
            "32298/32298 [==============================] - 12s 380us/sample - loss: 0.2566 - acc: 0.9083 - val_loss: 2.5877 - val_acc: 0.5015\n",
            "Epoch 67/100\n",
            "32298/32298 [==============================] - 12s 386us/sample - loss: 0.3278 - acc: 0.8814 - val_loss: 2.4009 - val_acc: 0.4865\n",
            "Epoch 68/100\n",
            "32298/32298 [==============================] - 12s 383us/sample - loss: 0.2997 - acc: 0.8893 - val_loss: 2.3722 - val_acc: 0.4987\n",
            "Epoch 69/100\n",
            "32298/32298 [==============================] - 13s 395us/sample - loss: 0.2970 - acc: 0.8940 - val_loss: 2.4735 - val_acc: 0.4921\n",
            "Epoch 70/100\n",
            "32298/32298 [==============================] - 12s 387us/sample - loss: 0.2792 - acc: 0.9000 - val_loss: 2.5111 - val_acc: 0.4851\n",
            "Epoch 71/100\n",
            "32298/32298 [==============================] - 12s 383us/sample - loss: 0.2809 - acc: 0.9006 - val_loss: 2.5185 - val_acc: 0.4815\n",
            "Epoch 72/100\n",
            "32298/32298 [==============================] - 12s 386us/sample - loss: 0.3149 - acc: 0.8849 - val_loss: 2.4187 - val_acc: 0.5071\n",
            "Epoch 73/100\n",
            "32298/32298 [==============================] - 13s 391us/sample - loss: 0.2439 - acc: 0.9121 - val_loss: 2.6808 - val_acc: 0.4781\n",
            "Epoch 74/100\n",
            "32298/32298 [==============================] - 12s 387us/sample - loss: 0.2297 - acc: 0.9167 - val_loss: 2.8613 - val_acc: 0.4589\n",
            "Epoch 75/100\n",
            "32298/32298 [==============================] - 12s 386us/sample - loss: 0.2234 - acc: 0.9201 - val_loss: 2.7852 - val_acc: 0.5007\n",
            "Epoch 76/100\n",
            "32298/32298 [==============================] - 12s 387us/sample - loss: 0.2123 - acc: 0.9220 - val_loss: 2.8417 - val_acc: 0.5007\n",
            "Epoch 77/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.2024 - acc: 0.9275 - val_loss: 2.8422 - val_acc: 0.5026\n",
            "Epoch 78/100\n",
            "32298/32298 [==============================] - 13s 387us/sample - loss: 0.2008 - acc: 0.9287 - val_loss: 2.8602 - val_acc: 0.4993\n",
            "Epoch 79/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.1949 - acc: 0.9310 - val_loss: 2.9036 - val_acc: 0.5035\n",
            "Epoch 80/100\n",
            "32298/32298 [==============================] - 12s 384us/sample - loss: 0.1779 - acc: 0.9360 - val_loss: 3.0019 - val_acc: 0.4985\n",
            "Epoch 81/100\n",
            "32298/32298 [==============================] - 13s 390us/sample - loss: 0.1653 - acc: 0.9423 - val_loss: 3.1011 - val_acc: 0.4862\n",
            "Epoch 82/100\n",
            "32298/32298 [==============================] - 13s 391us/sample - loss: 0.1790 - acc: 0.9363 - val_loss: 3.0372 - val_acc: 0.4976\n",
            "Epoch 83/100\n",
            "32298/32298 [==============================] - 13s 394us/sample - loss: 0.1675 - acc: 0.9401 - val_loss: 3.0378 - val_acc: 0.4996\n",
            "Epoch 84/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.1575 - acc: 0.9432 - val_loss: 3.0919 - val_acc: 0.4845\n",
            "Epoch 85/100\n",
            "32298/32298 [==============================] - 13s 391us/sample - loss: 0.1567 - acc: 0.9450 - val_loss: 3.0350 - val_acc: 0.5038\n",
            "Epoch 86/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.1484 - acc: 0.9483 - val_loss: 3.1767 - val_acc: 0.5010\n",
            "Epoch 87/100\n",
            "32298/32298 [==============================] - 13s 389us/sample - loss: 0.1409 - acc: 0.9497 - val_loss: 3.1495 - val_acc: 0.4982\n",
            "Epoch 88/100\n",
            "32298/32298 [==============================] - 13s 400us/sample - loss: 0.1378 - acc: 0.9512 - val_loss: 3.1920 - val_acc: 0.5032\n",
            "Epoch 89/100\n",
            "32298/32298 [==============================] - 13s 387us/sample - loss: 0.1427 - acc: 0.9498 - val_loss: 3.3164 - val_acc: 0.4912\n",
            "Epoch 90/100\n",
            "32298/32298 [==============================] - 12s 386us/sample - loss: 0.1247 - acc: 0.9554 - val_loss: 3.3029 - val_acc: 0.4990\n",
            "Epoch 91/100\n",
            "32298/32298 [==============================] - 13s 388us/sample - loss: 0.1292 - acc: 0.9540 - val_loss: 3.2202 - val_acc: 0.5026\n",
            "Epoch 92/100\n",
            "32298/32298 [==============================] - 13s 389us/sample - loss: 0.1252 - acc: 0.9549 - val_loss: 3.3343 - val_acc: 0.5046\n",
            "Epoch 93/100\n",
            "32298/32298 [==============================] - 13s 389us/sample - loss: 0.1174 - acc: 0.9579 - val_loss: 3.3696 - val_acc: 0.4937\n",
            "Epoch 94/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.1117 - acc: 0.9601 - val_loss: 3.3405 - val_acc: 0.5004\n",
            "Epoch 95/100\n",
            "32298/32298 [==============================] - 13s 388us/sample - loss: 0.1153 - acc: 0.9590 - val_loss: 3.4328 - val_acc: 0.5032\n",
            "Epoch 96/100\n",
            "32298/32298 [==============================] - 12s 385us/sample - loss: 0.1251 - acc: 0.9566 - val_loss: 3.3625 - val_acc: 0.5088\n",
            "Epoch 97/100\n",
            "32298/32298 [==============================] - 13s 387us/sample - loss: 0.0961 - acc: 0.9666 - val_loss: 3.4476 - val_acc: 0.5099\n",
            "Epoch 98/100\n",
            "32298/32298 [==============================] - 12s 387us/sample - loss: 0.1040 - acc: 0.9626 - val_loss: 3.4842 - val_acc: 0.5013\n",
            "Epoch 99/100\n",
            "32298/32298 [==============================] - 12s 383us/sample - loss: 0.1078 - acc: 0.9621 - val_loss: 3.5115 - val_acc: 0.5029\n",
            "Epoch 100/100\n",
            "32298/32298 [==============================] - 12s 386us/sample - loss: 0.1191 - acc: 0.9581 - val_loss: 3.4439 - val_acc: 0.5093\n",
            "Test loss: 3.4438643816886554\n",
            "Test accuracy: 0.5093341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY_kOK48ig83",
        "colab_type": "code",
        "outputId": "015d1327-ff80-41d9-eac9-be8919d32fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "checkpoint_file = path + 'pruned_mobilenet_checkpoint.h5'\n",
        "print('Saving pruned model to: ', checkpoint_file)\n",
        "# saved_model() sets include_optimizer to True by default. Spelling it out here\n",
        "# to highlight.\n",
        "tf.keras.models.save_model(pr_model, checkpoint_file, include_optimizer=True)\n",
        "\n",
        "with sparsity.prune_scope():\n",
        "  restored_model = tf.keras.models.load_model(checkpoint_file)\n",
        "\n",
        "restored_model.fit(x_train, y_train,\n",
        "                   batch_size=batch_size,\n",
        "                   epochs=10,\n",
        "                   verbose=1,\n",
        "                   callbacks=callbacks,\n",
        "                   validation_data=(x_test, y_test))\n",
        "\n",
        "score = restored_model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  drive/My Drive/interns/Kpit/pruned_mobilenet_checkpoint.h5\n",
            "Train on 32298 samples, validate on 3589 samples\n",
            "Epoch 1/10\n",
            "32298/32298 [==============================] - 23s 717us/sample - loss: 0.1035 - acc: 0.9633 - val_loss: 3.5311 - val_acc: 0.4773\n",
            "Epoch 2/10\n",
            "32298/32298 [==============================] - 11s 344us/sample - loss: 0.0891 - acc: 0.9685 - val_loss: 3.5037 - val_acc: 0.5155\n",
            "Epoch 3/10\n",
            "32298/32298 [==============================] - 11s 342us/sample - loss: 0.0916 - acc: 0.9658 - val_loss: 3.6746 - val_acc: 0.4990\n",
            "Epoch 4/10\n",
            "32298/32298 [==============================] - 11s 342us/sample - loss: 0.1041 - acc: 0.9631 - val_loss: 3.8436 - val_acc: 0.4606\n",
            "Epoch 5/10\n",
            "32298/32298 [==============================] - 11s 347us/sample - loss: 0.1024 - acc: 0.9642 - val_loss: 3.6854 - val_acc: 0.4804\n",
            "Epoch 6/10\n",
            "32298/32298 [==============================] - 11s 346us/sample - loss: 0.1027 - acc: 0.9624 - val_loss: 3.5733 - val_acc: 0.4960\n",
            "Epoch 7/10\n",
            "32298/32298 [==============================] - 11s 345us/sample - loss: 0.0899 - acc: 0.9681 - val_loss: 3.5244 - val_acc: 0.4974\n",
            "Epoch 8/10\n",
            "32298/32298 [==============================] - 11s 346us/sample - loss: 0.0928 - acc: 0.9663 - val_loss: 3.7754 - val_acc: 0.4765\n",
            "Epoch 9/10\n",
            "32298/32298 [==============================] - 11s 346us/sample - loss: 0.0879 - acc: 0.9688 - val_loss: 3.9138 - val_acc: 0.4419\n",
            "Epoch 10/10\n",
            "32298/32298 [==============================] - 11s 348us/sample - loss: 0.0990 - acc: 0.9629 - val_loss: 3.5007 - val_acc: 0.5132\n",
            "3589/3589 [==============================] - 3s 838us/sample - loss: 3.5007 - acc: 0.5132\n",
            "Test loss: 3.500702469619279\n",
            "Test accuracy: 0.5132349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaBoCaj1kDnD",
        "colab_type": "code",
        "outputId": "bb1cf138-b392-496f-e04b-ae814bfe5892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "final_model = sparsity.strip_pruning(pr_model)\n",
        "final_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 24, 24, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_13 (Depthwi (None, 24, 24, 32)        288       \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 24, 24, 64)        2048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_14 (Depthwi (None, 12, 12, 64)        576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 12, 12, 128)       8192      \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_15 (Depthwi (None, 12, 12, 128)       1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 12, 12, 128)       16384     \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_16 (Depthwi (None, 6, 6, 128)         1152      \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_34 (Activation)   (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 6, 6, 256)         32768     \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_35 (Activation)   (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_17 (Depthwi (None, 6, 6, 256)         2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_36 (Activation)   (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 6, 6, 256)         65536     \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_37 (Activation)   (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_18 (Depthwi (None, 3, 3, 256)         2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 3, 3, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 3, 3, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 3, 3, 512)         131072    \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_19 (Depthwi (None, 3, 3, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 3, 3, 512)         262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_20 (Depthwi (None, 3, 3, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 3, 3, 512)         262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_21 (Depthwi (None, 3, 3, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 3, 3, 512)         262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_22 (Depthwi (None, 3, 3, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 3, 3, 512)         262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_23 (Depthwi (None, 3, 3, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_48 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 3, 3, 512)         262144    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_49 (Activation)   (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_24 (Depthwi (None, 2, 2, 512)         4608      \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_50 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 2, 2, 1024)        524288    \n",
            "_________________________________________________________________\n",
            "batch_normalization_51 (Batc (None, 2, 2, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "activation_51 (Activation)   (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "depthwise_conv2d_25 (Depthwi (None, 2, 2, 1024)        9216      \n",
            "_________________________________________________________________\n",
            "batch_normalization_52 (Batc (None, 2, 2, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 2, 2, 1024)        1048576   \n",
            "_________________________________________________________________\n",
            "batch_normalization_53 (Batc (None, 2, 2, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 2, 2, 1024)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 7175      \n",
            "=================================================================\n",
            "Total params: 3,235,495\n",
            "Trainable params: 3,213,607\n",
            "Non-trainable params: 21,888\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7sB6FKwk_NO",
        "colab_type": "code",
        "outputId": "f91d25cf-94dc-493f-fccd-cbfa5af5c94e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pruned_keras_file = path + 'pruned_mobilenet.h5'\n",
        "print('Saving pruned model to: ', pruned_keras_file)\n",
        "\n",
        "# No need to save the optimizer with the graph for serving.\n",
        "tf.keras.models.save_model(final_model, pruned_keras_file, include_optimizer=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving pruned model to:  drive/My Drive/interns/Kpit/pruned_mobilenet.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIcU6y6Vl-ur",
        "colab_type": "code",
        "outputId": "1daeb083-8e53-44c0-9c3b-8685acd41eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "zip1 = path + 'raw_mobilenet.zip' \n",
        "with zipfile.ZipFile(zip1, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(keras_file)\n",
        "print(\"Size of the unpruned model before compression: %.2f Mb\" % \n",
        "      (os.path.getsize(keras_file) / float(2**20)))\n",
        "print(\"Size of the unpruned model after compression: %.2f Mb\" % \n",
        "      (os.path.getsize(zip1) / float(2**20)))\n",
        "\n",
        "zip2 = path + 'pruned_mobilenet.zip' \n",
        "with zipfile.ZipFile(zip2, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(pruned_keras_file)\n",
        "print(\"Size of the pruned model before compression: %.2f Mb\" % \n",
        "      (os.path.getsize(pruned_keras_file) / float(2**20)))\n",
        "print(\"Size of the pruned model after compression: %.2f Mb\" % \n",
        "      (os.path.getsize(zip2) / float(2**20)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the unpruned model before compression: 12.57 Mb\n",
            "Size of the unpruned model after compression: 11.49 Mb\n",
            "Size of the pruned model before compression: 12.57 Mb\n",
            "Size of the pruned model after compression: 2.67 Mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3WfIEe6mi5z",
        "colab_type": "code",
        "outputId": "12844730-df51-4e10-f635-aca4e3e2c475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tflite_model_file = path + 'sparse_mobilenet.tflite'\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model_file(pruned_keras_file)\n",
        "tflite_model = converter.convert()\n",
        "with open(tflite_model_file, 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0710 19:52:47.943387 140356938360704 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSHTaNRCpAWD",
        "colab_type": "code",
        "outputId": "24aba9b4-5f6b-466f-8bda-af50d5c70a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "zip_tflite = path + 'pruned_mobilenet_tflite.zip'\n",
        "with zipfile.ZipFile(zip_tflite, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(tflite_model_file)\n",
        "print(\"Size of the tflite model before compression: %.2f Mb\" \n",
        "      % (os.path.getsize(tflite_model_file) / float(2**20)))\n",
        "print(\"Size of the tflite model after compression: %.2f Mb\" \n",
        "      % (os.path.getsize(zip_tflite) / float(2**20)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the tflite model before compression: 12.23 Mb\n",
            "Size of the tflite model after compression: 2.53 Mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grsD0JybpQcE",
        "colab_type": "code",
        "outputId": "f7a3ef27-0790-4972-aadd-3a0a0ca53be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "def eval_model(interpreter, x_test, y_test):\n",
        "  total_seen = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  for img, label in zip(x_test, y_test):\n",
        "    inp = img.reshape((1, 48, 48, 1))\n",
        "    total_seen += 1\n",
        "    interpreter.set_tensor(input_index, inp)\n",
        "    interpreter.invoke()\n",
        "    predictions = interpreter.get_tensor(output_index)\n",
        "    if np.argmax(predictions) == np.argmax(label):\n",
        "      num_correct += 1\n",
        "\n",
        "    if total_seen % 1000 == 0:\n",
        "        print(\"Accuracy after %i images: %f\" %\n",
        "              (total_seen, float(num_correct) / float(total_seen)))\n",
        "\n",
        "  return float(num_correct) / float(total_seen)\n",
        "\n",
        "\n",
        "print(eval_model(interpreter, x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy after 1000 images: 0.489000\n",
            "Accuracy after 2000 images: 0.497000\n",
            "Accuracy after 3000 images: 0.505667\n",
            "0.5093340763443857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_7dGon9prTl",
        "colab_type": "code",
        "outputId": "ae087138-c2a6-450a-fa0c-bbea779556ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model_file(pruned_keras_file)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "tflite_quant_model_file = path + 'sparse_mobilenet_quant.tflite'\n",
        "with open(tflite_quant_model_file, 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0710 19:55:55.936868 140356938360704 hdf5_format.py:171] No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCaufm0bqaWM",
        "colab_type": "code",
        "outputId": "77e2ff80-22a3-4de6-919b-5f457bea98cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "zip_tflite = path + 'pruned_quant_mobilenet_tflite.zip'\n",
        "with zipfile.ZipFile(zip_tflite, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "  f.write(tflite_quant_model_file)\n",
        "print(\"Size of the tflite model before compression: %.2f Mb\" \n",
        "      % (os.path.getsize(tflite_quant_model_file) / float(2**20)))\n",
        "print(\"Size of the tflite model after compression: %.2f Mb\" \n",
        "      % (os.path.getsize(zip_tflite) / float(2**20)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the tflite model before compression: 3.10 Mb\n",
            "Size of the tflite model after compression: 0.69 Mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q30m5CqyqmN9",
        "colab_type": "code",
        "outputId": "481ce063-b157-47e0-d0d2-68da994251cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_quant_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "st = time.time()\n",
        "print(eval_model(interpreter, x_test, y_test))\n",
        "end = time.time()\n",
        "print(end - st)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy after 1000 images: 0.487000\n",
            "Accuracy after 2000 images: 0.492000\n",
            "Accuracy after 3000 images: 0.501667\n",
            "0.5054332683198662\n",
            "20.64324688911438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysadLBpftDS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_quant_model_file))\n",
        "interpreter.allocate_tensors()\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "num_correct = 0\n",
        "aa = time.time()\n",
        "for img,label in zip(x_test, y_test):\n",
        "    inp = img.reshape((1,48,48,1))\n",
        "    interpreter.set_tensor(input_index, inp)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_index)\n",
        "    if np.argmax(output) == np.argmax(label):\n",
        "        num_correct = num_correct + 1\n",
        "time_elapsed = time.time() - aa\n",
        "\n",
        "print('total_images : ', len(x_test), 'accuracy : ' , float(num_correct)/len(x_test))\n",
        "print('total time :', time_elapsed , 'time per image: ', time_elapsed/len(x_test), 'fps : ', len(x_test)/time_elapsed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}